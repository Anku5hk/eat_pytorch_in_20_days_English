# 5.3, loss function losses

Generally speaking, the objective function of supervised learning consists of a loss function and a regularization term. (Objective = Loss + Regularization)

The loss function in Pytorch is generally specified when training the model.

Note that the parameters of the built-in loss function in Pytorch are different from tensorflow, y_pred is in the front, y_true is in the back, and Tensorflow is y_true in the front and y_pred is in the back.

For regression models, the commonly used built-in loss function is the mean square loss function nn.MSELoss.

For the binary classification model, the binary cross entropy loss function nn.BCELoss is usually used (the input is already the result of the sigmoid activation function)
Or nn.BCEWithLogitsLoss (the input has not passed the nn.Sigmoid activation function).

For multi-class models, it is generally recommended to use the cross-entropy loss function nn.CrossEntropyLoss.
(y_true needs to be one-dimensional and is a category code. y_pred is not activated by nn.Softmax.)

In addition, if the multi-class y_pred has been activated by nn.LogSoftmax, the nn.NLLLoss loss function (The negative log likelihood loss) can be used.
This method is equivalent to using nn.CrossEntropyLoss directly.


If necessary, you can also customize the loss function. The custom loss function needs to receive two tensors y_pred and y_true as input parameters, and output a scalar as the loss function value.

The regularization item in Pytorch is generally added as the objective function along with the loss function in a custom way.

If only L2 regularization is used, the weight_decay parameter of the optimizer can also be used to achieve the same effect.



### One, built-in loss function

```python
import numpy as np
import pandas as pd
import torch
from torch import nn
import torch.nn.functional as F


y_pred = torch.tensor([[10.0,0.0,-10.0],[8.0,8.0,8.0]])
y_true = torch.tensor([0,2])

# Directly call the cross entropy loss
ce = nn.CrossEntropyLoss()(y_pred,y_true)
print(ce)

# It is equivalent to calculating nn.LogSoftmax activation first, and then calling NLLLoss
y_pred_logsoftmax = nn.LogSoftmax(dim = 1)(y_pred)
nll = nn.NLLLoss()(y_pred_logsoftmax,y_true)
print(nll)

```

```
tensor(0.5493)
tensor(0.5493)
```



The built-in loss function generally has two forms: class realization and function realization.

For example: nn.BCE and F.binary_cross_entropy are both binary cross entropy loss functions, the former is the realization form of the class, and the latter is the realization form of the function.

In fact, the realization form of the class is usually obtained by calling the realization form of the function and encapsulating it with nn.Module.

Generally, what we commonly use is the realization form of the class. They are encapsulated in the torch.nn module, and the class name ends with Loss.

Some commonly used built-in loss functions are described below.


* nn.MSELoss (mean square error loss, also called L2 loss, used for regression)

* nn.L1Loss (L1 loss, also called absolute value error loss, used for regression)

* nn.SmoothL1Loss (Smooth L1 loss, when the input is between -1 and 1, smoothing is L2 loss, used for regression)

* nn.BCELoss (binary cross entropy, used for binary classification, the input has been activated by nn.Sigmoid, and the weigths parameter can be used to adjust the category weight for unbalanced data sets)

* nn.BCEWithLogitsLoss (binary cross entropy, used for binary classification, the input is not activated by nn.Sigmoid)

* nn.CrossEntropyLoss (Cross Entropy, used for multi-classification, requires label to be sparse coding, input has not been activated by nn.Softmax, for imbalanced data sets, you can use the weigths parameter to adjust the category weight)

* nn.NLLLoss (negative log-likelihood loss, used for multi-classification, requires label to be sparse coding, input is activated by nn.LogSoftmax)

* nn.CosineSimilarity (cosine similarity, can be used for multiple classification)

* nn.AdaptiveLogSoftmaxWithLoss (a loss function suitable for very many categories and uneven distribution of categories, which will adaptively combine multiple small categories into a cluster)


For more information about the loss function, please refer to the following article:

"Eighteen Loss Functions of PyTorch"

https://zhuanlan.zhihu.com/p/61379965


### Second, custom loss function


The custom loss function receives two tensors y_pred, y_true as input parameters, and outputs a scalar as the loss function value.

It is also possible to subclass nn.Module and rewrite the forward method to realize the calculation logic of the loss, thereby obtaining the realization of the loss function class.

The following is a demonstration of a custom implementation of Focal Loss. Focal Loss is an improved loss function form of binary_crossentropy.

It has obvious advantages over binary_crossentropy when the samples are unbalanced and there are many easy-to-classify samples.

It has two adjustable parameters, alpha parameter and gamma parameter. The alpha parameter is mainly used to attenuate the weight of negative samples, and the gamma parameter is mainly used to attenuate the weight of easy-to-train samples.

This allows the model to focus more on positive samples and difficult samples. This is why this loss function is called Focal Loss.

For details, see "Understanding Focal Loss and GHM in 5 Minutes-A Tool for Solving Sample Imbalance"

https://zhuanlan.zhihu.com/p/80594704



$$focal\_loss(y,p) =
\begin{cases} -\alpha (1-p)^{\gamma}\log(p) & \text{if y = 1}\\
-(1-\alpha) p^{\gamma}\log(1-p) & \text{if y = 0}
\end{cases} $$

```python
class FocalLoss(nn.Module):
    
    def __init__(self,gamma=2.0,alpha=0.75):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self,y_pred,y_true):
        bce = torch.nn.BCELoss(reduction = "none")(y_pred,y_true)
        p_t = (y_true * y_pred) + ((1-y_true) * (1-y_pred))
        alpha_factor = y_true * self.alpha + (1-y_true) * (1-self.alpha)
        modulating_factor = torch.pow(1.0-p_t, self.gamma)
        loss = torch.mean(alpha_factor * modulating_factor * bce)
        return loss
    
    
    
```

```python
#Difficult sample
y_pred_hard = torch.tensor([[0.5],[0.5]])
y_true_hard = torch.tensor([[1.0],[0.0]])

#Easy sample
y_pred_easy = torch.tensor([[0.9],[0.1]])
y_true_easy = torch.tensor([[1.0],[0.0]])

focal_loss = FocalLoss()
bce_loss = nn.BCELoss()

print("focal_loss(hard samples):", focal_loss(y_pred_hard,y_true_hard))
print("bce_loss(hard samples):", bce_loss(y_pred_hard,y_true_hard))
print("focal_loss(easy samples):", focal_loss(y_pred_easy,y_true_easy))
print("bce_loss(easy samples):", bce_loss(y_pred_easy,y_true_easy))

#Visible focal_loss makes the weight of the easy sample attenuate to the original 0.0005/0.1054 = 0.00474
# And let the weight of the difficult sample only decay to the original 0.0866/0.6931=0.12496

# Therefore, relatively speaking, focal_loss can attenuate the weight of easy samples.



```

```
focal_loss(hard samples): tensor(0.0866)
bce_loss(hard samples): tensor(0.6931)
focal_loss(easy samples): tensor(0.0005)
bce_loss(easy samples): tensor(0.1054)
```


For a complete example of using FocalLoss, please refer to the example in `Custom L1 and L2 regularization items` below. This example demonstrates the method of customizing regularization items as well as the usage of FocalLoss.



### Three, custom L1 and L2 regularization items


It is generally believed that L1 regularization can generate a sparse weight matrix, that is, a sparse model that can be used for feature selection.

The L2 regularization can prevent the model from overfitting. To a certain extent, L1 can also prevent overfitting.

Let's take a binary classification problem as an example to demonstrate the method of adding custom L1 and L2 regularization terms to the objective function of the model.

This example also demonstrates the use of FocalLoss in the previous part.




**1, prepare data**

```python
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader,TensorDataset
import torchkeras
%matplotlib inline
%config InlineBackend.figure_format ='svg'

#Number of positive and negative samples
n_positive,n_negative = 200,6000

#Generate positive samples, small circle distribution
r_p = 5.0 + torch.normal(0.0,1.0,size = [n_positive,1])
theta_p = 2*np.pi*torch.rand([n_positive,1])
Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = 1)
Yp = torch.ones_like(r_p)

#Generate negative samples, large circle distribution
r_n = 8.0 + torch.normal(0.0,1.0,size = [n_negative,1])
theta_n = 2*np.pi*torch.rand([n_negative,1])
Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = 1)
Yn = torch.zeros_like(r_n)

#Summary sample
X = torch.cat([Xp,Xn],axis = 0)
Y = torch.cat([Yp,Yn],axis = 0)


#Visualization
plt.figure(figsize = (6,6))
plt.scatter(Xp[:,0],Xp[:,1],c = "r")
plt.scatter(Xn[:,0],Xn[:,1],c = "g")
plt.legend(["positive","negative"]);

```

![](./data/5-3-åŒå¿ƒåœ†åˆ†å¸ƒ.png)

```python
ds = TensorDataset(X,Y)

ds_train,ds_valid = torch.utils.data.random_split(ds,[int(len(ds)*0.7),len(ds)-int(len(ds)*0.7)])
dl_train = DataLoader(ds_train,batch_size = 100,shuffle=True,num_workers=2)
dl_valid = DataLoader(ds_valid,batch_size = 100,num_workers=2)

```

**2, define the model**

```python
class DNNModel(torchkeras.Model):
    def __init__(self):
        super(DNNModel, self).__init__()
        self.fc1 = nn.Linear(2,4)
        self.fc2 = nn.Linear(4,8)
        self.fc3 = nn.Linear(8,1)
        
    def forward(self,x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        y = nn.Sigmoid()(self.fc3(x))
        return y
        
model = DNNModel()

model.summary(input_shape = (2,))
```

```
-------------------------------------------------- --------------
        Layer (type) Output Shape Param #
================================================= ==============
            Linear-1 [-1, 4] 12
            Linear-2 [-1, 8] 40
            Linear-3 [-1, 1] 9
================================================= ==============
Total params: 61
Trainable params: 61
Non-trainable params: 0
-------------------------------------------------- --------------
Input size (MB): 0.000008
Forward/backward pass size (MB): 0.000099
Params size (MB): 0.000233
Estimated Total Size (MB): 0.000340
-------------------------------------------------- --------------
```


**3, training model**

```python
# Accuracy
def accuracy(y_pred,y_true):
    y_pred = torch.where(y_pred>0.5,torch.ones_like(y_pred,dtype = torch.float32),
                      torch.zeros_like(y_pred,dtype = torch.float32))
    acc = torch.mean(1-torch.abs(y_true-y_pred))
    return acc

# L2 regularization
def L2Loss(model,alpha):
    l2_loss = torch.tensor(0.0, requires_grad=True)
    for name, param in model.named_parameters():
        if'bias' not in name: #Generally do not use regularity for bias items
            l2_loss = l2_loss + (0.5 * alpha * torch.sum(torch.pow(param, 2)))
    return l2_loss

# L1 regularization
def L1Loss(model,beta):
    l1_loss = torch.tensor(0.0, requires_grad=True)
    for name, param in model.named_parameters():
        if'bias' not in name:
            l1_loss = l1_loss + beta * torch.sum(torch.abs(param))
    return l1_loss

# Add L2 regular and L1 regular to FocalLoss loss, together as the objective function
def focal_loss_with_regularization(y_pred,y_true):
    focal = FocalLoss()(y_pred,y_true)
    l2_loss = L2Loss(model,0.001) #Pay attention to setting the regularization term coefficient
    l1_loss = L1Loss(model,0.001)
    total_loss = focal + l2_loss + l1_loss
    return total_loss

model.compile(loss_func =focal_loss_with_regularization,
              optimizer = torch.optim.Adam(model.parameters(),lr = 0.01),
             metrics_dict={"accuracy":accuracy})

dfhistory = model.fit(30,dl_train = dl_train,dl_val = dl_valid,log_step_freq = 30)

```

```
Start Training ...

================================================= ==============================2020-07-11 23:34:17
{'step': 30,'loss': 0.021,'accuracy': 0.972}

 +-------+-------+----------+----------+----------- ---+
| epoch | loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+----------- ---+
| 1 | 0.022 | 0.971 | 0.025 | 0.96 |
+-------+-------+----------+----------+----------- ---+

================================================= ==============================2020-07-11 23:34:27
{'step': 30,'loss': 0.016,'accuracy': 0.984}

 +-------+-------+----------+----------+----------- ---+
| epoch | loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+----------- ---+
| 30 | 0.016 | 0.981 | 0.017 | 0.983 |
+-------+-------+----------+----------+----------- ---+

================================================= ==============================2020-07-11 23:34:27
Finished Training...
```

```python

```

```python
# Result visualization
fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize = (12,5))
ax1.scatter(Xp[:,0],Xp[:,1], c="r")
ax1.scatter(Xn[:,0],Xn[:,1],c = "g")
ax1.legend(["positive","negative"]);
ax1.set_title("y_true");

Xp_pred = X[torch.squeeze(model.forward(X)>=0.5)]
Xn_pred = X[torch.squeeze(model.forward(X)<0.5)]

ax2.scatter(Xp_pred[:,0],Xp_pred[:,1],c = "r")
ax2.scatter(Xn_pred[:,0],Xn_pred[:,1],c = "g")
ax2.legend(["positive","negative"]);
ax2.set_title("y_pred");

```

![](./data/5-3-focal_lossé¢„æµ‹ç»“æžœ.png)

```python

```

### Fourth, realize L2 regularization through the optimizer


If you only need to use L2 regularization, you can also use the optimizer's weight_decay parameter to achieve.

The weight_decay parameter can set the attenuation of the parameter during training, which is equivalent to the effect of L2 regularization.


```
before L2 regularization:

gradient descent: w = w-lr * dloss_dw

after L2 regularization:

gradient descent: w = w-lr * (dloss_dw+beta*w) = (1-lr*beta)*w-lr*dloss_dw

so (1-lr*beta) is the weight decay ratio.
```


Pytorch's optimizer supports an operation called Per-parameter options, which is to specify a specific learning rate and weight decay rate for each parameter to meet more detailed requirements.

```python
weight_params = [param for name, param in model.named_parameters() if "bias" not in name]
bias_params = [param for name, param in model.named_parameters() if "bias" in name]

optimizer = torch.optim.SGD([{'params': weight_params,'weight_decay':1e-5},
                             {'params': bias_params,'weight_decay':0}],
                            lr=1e-2, momentum=0.9)

```

```python

```

**If this book is helpful to you and want to encourage the author, remember to add a starâ­ï¸ to this project and share it with your friends ðŸ˜Š!**

If you need to further communicate with the author on the understanding of the content of this book, please leave a message under the public account "Algorithm Food House". The author has limited time and energy and will respond as appropriate.

You can also reply to keywords in the background of the official account: **Add group**, join the reader exchange group and discuss with you.

![ç®—æ³•ç¾Žé£Ÿå±‹logo.png](./data/ç®—æ³•ç¾Žé£Ÿå±‹äºŒç»´ç .jpg)

```python

```
