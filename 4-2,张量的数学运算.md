# 4-2, Mathematical operations on tensors

The operations of tensors mainly include structural operations of tensors and mathematical operations of tensors.

Tensor structure operations such as: tensor creation, index slicing, dimension transformation, merge and split.

Tensor mathematical operations mainly include: scalar operations, vector operations, and matrix operations. In addition, we will introduce the broadcasting mechanism of tensor operations.

In this article, we introduce the mathematical operations of tensors.

Part of the content of this article refers to the following blog: https://blog.csdn.net/duan_zhihua/article/details/82526505

```python

```

### One, scalar operation


The mathematical operators of tensors can be divided into scalar operators, vector operators, and matrix operators.

Addition, subtraction, multiplication and division, as well as common functions such as trigonometric functions, exponents, logarithms, and logical comparison operators are all scalar operators.

The characteristic of scalar operators is to perform element-wise operations on tensors.

Some scalar operators have overloaded commonly used mathematical operators. And it supports broadcasting features similar to numpy.

```python
import torch
import numpy as np
```

```python
a = torch.tensor([[1.0,2],[-3,4.0]])
b = torch.tensor([[5.0,6],[7.0,8.0]])
a+b #Operator overload
```

```
tensor([[ 6., 8.],
        [4., 12.]])
```

```python
a-b
```

```
tensor([[ -4., -4.],
        [-10., -4.]])
```

```python
a*b
```

```
tensor([[ 5., 12.],
        [-21., 32.]])
```

```python
a/b
```

```
tensor([[ 0.2000, 0.3333],
        [-0.4286, 0.5000]])
```

```python
a**2
```

```
tensor([[ 1., 4.],
        [9., 16.]])
```

```python
a**(0.5)
```

```
tensor([[1.0000, 1.4142],
        [nan, 2.0000]])
```

```python
a%3 #Modulo
```

```
tensor([[1., 2.],
        [0., 1.]])
```

```python
a//3 #Floor division
```

```
tensor([[ 0., 0.],
        [-1., 1.]])
```

```python
a>=2 # torch.ge(a,2) #ge: greater_equal abbreviation
```

```
tensor([[False, True],
        [False, True]])
```

```python
(a>=2)&(a<=3)
```

```
tensor([[False, True],
        [False, False]])
```

```python
(a>=2)|(a<=3)
```

```
tensor([[True, True],
        [True, True]])
```

```python
a==5 #torch.eq(a,5)
```

```
tensor([[False, False],
        [False, False]])
```

```python
torch.sqrt(a)
```

```
tensor([[1.0000, 1.4142],
        [nan, 2.0000]])
```

```python
a = torch.tensor([1.0,8.0])
b = torch.tensor([5.0,6.0])
c = torch.tensor([6.0,7.0])

d = a+b+c
print(d)
```

```
tensor([12., 21.])
```

```python
print(torch.max(a,b))
```

```
tensor([5., 8.])
```

```python
print(torch.min(a,b))
```

```
tensor([1., 6.])
```

```python
x = torch.tensor([2.6,-2.7])

print(torch.round(x)) #Keep the integer part, round off
print(torch.floor(x)) #Keep the integer part, round down
print(torch.ceil(x)) #Keep the integer part and round up
print(torch.trunc(x)) #Keep the integer part and round to 0
```

```
tensor([ 3., -3.])
tensor([ 2., -3.])
tensor([ 3., -2.])
tensor([ 2., -2.])
```

```python
x = torch.tensor([2.6,-2.7])
print(torch.fmod(x,2)) #do the division and take the remainder
print(torch.remainder(x,2)) #do the division and take the remaining part, the result is always positive
```

```
tensor([ 0.6000, -0.7000])
tensor([0.6000, 1.3000])
```

```python
# Amplitude clipping
x = torch.tensor([0.9,-0.8,100.0,-20.0,0.7])
y = torch.clamp(x,min=-1,max = 1)
z = torch.clamp(x,max = 1)
print(y)
print(z)
```

```
tensor([ 0.9000, -0.8000, 1.0000, -1.0000, 0.7000])
tensor([ 0.9000, -0.8000, 1.0000, -20.0000, 0.7000])
```

```python

```

### Two, vector operations


Vector operators only operate on a specific axis, mapping a vector to a scalar or another vector.


```python
#Statistics

a = torch.arange(1,10).float()
print(torch.sum(a))
print(torch.mean(a))
print(torch.max(a))
print(torch.min(a))
print(torch.prod(a)) #cumulative multiplication
print(torch.std(a)) #standard deviation
print(torch.var(a)) #variance
print(torch.median(a)) #Median

```

```
tensor(45.)
tensor(5.)
tensor(9.)
tensor(1.)
tensor(362880.)
tensor(2.7386)
tensor(7.5000)
tensor(5.)

```

```python
#Specify the dimension to calculate the statistical value

b = a.view(3,3)
print(b)
print(torch.max(b,dim = 0))
print(torch.max(b,dim = 1))
```

```python

```

```
tensor([[1., 2., 3.],
        [4., 5., 6.],
        [7., 8., 9.]])
torch.return_types.max(
values=tensor([7., 8., 9.]),
indices=tensor([2, 2, 2]))
torch.return_types.max(
values=tensor([3., 6., 9.]),
indices=tensor([2, 2, 2]))
```

```python
#cumscan
a = torch.arange(1,10)

print(torch.cumsum(a,0))
print(torch.cumprod(a,0))
print(torch.cummax(a,0).values)
print(torch.cummax(a,0).indices)
print(torch.cummin(a,0))
```

```
tensor([ 1, 3, 6, 10, 15, 21, 28, 36, 45])
tensor([ 1, 2, 6, 24, 120, 720, 5040, 40320, 362880])
tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])
torch.return_types.cummin(
values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),
indices=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))
```

```python
#torch.sort and torch.topk can sort tensors
a = torch.tensor([[9,7,8],[1,3,2],[5,6,4]]).float()
print(torch.topk(a,2,dim = 0),"\n")
print(torch.topk(a,2,dim = 1),"\n")
print(torch.sort(a,dim = 1),"\n")

#Use torch.topk to implement KNN algorithm in Pytorch
```

```
torch.return_types.topk(
values=tensor([[9., 7., 8.],
        [5., 6., 4.]]),
indices=tensor([[0, 0, 0],
        [2, 2, 2]]))

torch.return_types.topk(
values=tensor([[9., 8.],
        [3., 2.],
        [6., 5.]]),
indices=tensor([[0, 2],
        [1, 2],
        [1, 0]]))

torch.return_types.sort(
values=tensor([[7., 8., 9.],
        [1., 2., 3.],
        [4., 5., 6.]]),
indices=tensor([[1, 2, 0],
        [0, 2, 1],
        [2, 0, 1]]))

```

```python

```

### Three, matrix operations


The matrix must be two-dimensional. Things like torch.tensor([1,2,3]) are not matrices.

Matrix operations include: matrix multiplication, matrix transposition, matrix inversion, matrix trace, matrix norm, matrix determinant, matrix eigenvalue, matrix decomposition and other operations.


```python
#Matrix multiplication
a = torch.tensor([[1,2],[3,4]])
b = torch.tensor([[2,0],[0,2]])
print(a@b) #Equivalent to torch.matmul(a,b) or torch.mm(a,b)
```

```
tensor([[2, 4],
        [6, 8]])
```

```python
#Matrix transpose
a = torch.tensor([[1.0,2],[3,4]])
print(a.t())
```

```
tensor([[1., 3.],
        [twenty four.]])
```

```python
#Matrix inverse, must be a floating point type
a = torch.tensor([[1.0,2],[3,4]])
print(torch.inverse(a))
```

```
tensor([[-2.0000, 1.0000],
        [1.5000, -0.5000]])
```

```python
#Matrix seeking trace
a = torch.tensor([[1.0,2],[3,4]])
print(torch.trace(a))
```

```
tensor(5.)
```

```python
#Matrix norm
a = torch.tensor([[1.0,2],[3,4]])
print(torch.norm(a))
```

```
tensor(5.4772)
```

```python
#Matrix determinant
a = torch.tensor([[1.0,2],[3,4]])
print(torch.det(a))
```

```
tensor(-2.0000)
```

```python
#Matrix eigenvalues ​​and eigenvectors
a = torch.tensor([[1.0,2],[-5,4]],dtype = torch.float)
print(torch.eig(a,eigenvectors=True))

#The two eigenvalues ​​are -2.5+2.7839j, 2.5-2.7839j
```

```
torch.return_types.eig(
eigenvalues=tensor([[ 2.5000, 2.7839],
        [2.5000, -2.7839]]),
eigenvectors=tensor([[ 0.2535, -0.4706],
        [0.8452, 0.0000]]))
```

```python
#Matrix QR decomposition, decompose a square matrix into an orthogonal matrix q and upper triangular matrix r
#QR decomposition actually implements Schmidt orthogonalization of matrix a to obtain q

a = torch.tensor([[1.0,2.0],[3.0,4.0]])
q,r = torch.qr(a)
print(q,"\n")
print(r,"\n")
print(q@r)
```

```python

```

```python
#Matrix svd decomposition
#svd decomposition can decompose any matrix into an orthogonal matrix u, the product of a diagonal matrix s and an orthogonal matrix v.t()
#svd is often used for matrix compression and dimensionality reduction
a=torch.tensor([[1.0,2.0],[3.0,4.0],[5.0,6.0]])

u,s,v = torch.svd(a)

print(u,"\n")
print(s,"\n")
print(v,"\n")

print(u@torch.diag(s)@v.t())

#Using svd decomposition can realize principal component analysis dimensionality reduction in Pytorch

```

```
tensor([[-0.2298, 0.8835],
        [-0.5247, 0.2408],
        [-0.8196, -0.4019]])

tensor([9.5255, 0.5143])

tensor([[-0.6196, -0.7849],
        [-0.7849, 0.6196]])

tensor([[1.0000, 2.0000],
        [3.0000, 4.0000],
        [5.0000, 6.0000]])
```

```python

```

```python

```

### Fourth, broadcast mechanism


Pytorch's broadcasting rules are the same as numpy:

* 1. If the dimensions of the tensors are different, expand the tensor with the smaller dimension until the dimensions of the two tensors are the same.
* 2. If the length of two tensors in a certain dimension is the same, or the length of one of the tensors in this dimension is 1, then we say that the two tensors are compatible in this dimension .
* 3. If two tensors are compatible in all dimensions, they can use broadcasting.
* 4. After broadcasting, the length of each dimension will take the larger value of the two tensors in that dimension.
* 5. In any dimension, if the length of one tensor is 1 and the length of the other tensor is greater than 1, then in that dimension, it is as if the first tensor is copied.

torch.broadcast_tensors can convert multiple tensors into the same dimension according to broadcast rules.

```python
a = torch.tensor([1,2,3])
b = torch.tensor([[0,0,0],[1,1,1],[2,2,2]])
print(b + a)
```

```
tensor([[1, 2, 3],
        [2, 3, 4],
        [3, 4, 5]])
```

```python
a_broad,b_broad = torch.broadcast_tensors(a,b)
print(a_broad,"\n")
print(b_broad,"\n")
print(a_broad + b_broad)
```

```
tensor([[1, 2, 3],
        [1, 2, 3],
        [1, 2, 3]])

tensor([[0, 0, 0],
        [1, 1, 1],
        [2, 2, 2]])

tensor([[1, 2, 3],
        [2, 3, 4],
        [3, 4, 5]])
```

```python

```

**If this book is helpful to you and want to encourage the author, remember to add a star to this project, and share it with your friends 😊!**

If you need to further communicate with the author on the understanding of the content of this book, please leave a message under the public account "Algorithm Food House". The author has limited time and energy and will respond as appropriate.

You can also reply to keywords in the background of the official account: **Add group**, join the reader exchange group and discuss with you.

![算法美食屋logo.png](./data/算法美食屋二维码.jpg)

```python

```
