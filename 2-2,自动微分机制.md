# 2-2, automatic differentiation mechanism


Neural networks usually rely on back propagation to find gradients to update network parameters. The process of finding gradients is usually a very complicated and error-prone thing.

The deep learning framework can help us automatically complete this gradient calculation.

Pytorch generally implements this gradient calculation through the backward method of backpropagation. The gradient obtained by this method will be stored under the grad attribute of the corresponding independent variable tensor.

In addition, the torch.autograd.grad function can also be called to achieve gradient calculation.

This is Pytorch's automatic differentiation mechanism.


### One, use the backward method to find the derivative


The backward method is usually called on a scalar tensor, and the gradient obtained by this method will be stored under the grad property of the corresponding independent variable tensor.

If the called tensor is non-scalar, a gradient parameter tensor with the same shape as it should be passed in.

It is equivalent to using the gradient parameter tensor and the calling tensor as a vector dot product, and the obtained scalar result is then backpropagated.



**1, backpropagation of scalar**

```python
import numpy as np
import torch

# f(x) = a*x**2 + b*x + c derivative

x = torch.tensor(0.0,requires_grad = True) # x needs to be differentiated
a = torch.tensor(1.0)
b = torch.tensor(-2.0)
c = torch.tensor(1.0)
y = a*torch.pow(x,2) + b*x + c

y.backward()
dy_dx = x.grad
print(dy_dx)

```

```
tensor(-2.)
```


**2, non-scalar backpropagation**

```python
import numpy as np
import torch

# f(x) = a*x**2 + b*x + c

x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True) # x needs to be differentiated
a = torch.tensor(1.0)
b = torch.tensor(-2.0)
c = torch.tensor(1.0)
y = a*torch.pow(x,2) + b*x + c

gradient = torch.tensor([[1.0,1.0],[1.0,1.0]])

print("x:\n",x)
print("y:\n",y)
y.backward(gradient = gradient)
x_grad = x.grad
print("x_grad:\n",x_grad)
```

```
x:
 tensor([[0., 0.],
        [1., 2.]], requires_grad=True)
y:
 tensor([[1., 1.],
        [0., 1.]], grad_fn=<AddBackward0>)
x_grad:
 tensor([[-2., -2.],
        [0., 2.]])
```


**3, non-scalar backpropagation can be realized by scalar backpropagation**

```python
import numpy as np
import torch

# f(x) = a*x**2 + b*x + c

x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True) # x needs to be differentiated
a = torch.tensor(1.0)
b = torch.tensor(-2.0)
c = torch.tensor(1.0)
y = a*torch.pow(x,2) + b*x + c

gradient = torch.tensor([[1.0,1.0],[1.0,1.0]])
z = torch.sum(y*gradient)

print("x:",x)
print("y:",y)
z.backward()
x_grad = x.grad
print("x_grad:\n",x_grad)
```

```
x: tensor([[0., 0.],
        [1., 2.]], requires_grad=True)
y: tensor([[1., 1.],
        [0., 1.]], grad_fn=<AddBackward0>)
x_grad:
 tensor([[-2., -2.],
        [0., 2.]])
```

```python

```

### Second, use the autograd.grad method to find the derivative

```python
import numpy as np
import torch

# f(x) = a*x**2 + b*x + c derivative

x = torch.tensor(0.0,requires_grad = True) # x needs to be differentiated
a = torch.tensor(1.0)
b = torch.tensor(-2.0)
c = torch.tensor(1.0)
y = a*torch.pow(x,2) + b*x + c


# create_graph set to True will allow the creation of higher order derivatives
dy_dx = torch.autograd.grad(y,x,create_graph=True)[0]
print(dy_dx.data)

# Find the second derivative
dy2_dx2 = torch.autograd.grad(dy_dx,x)[0]

print(dy2_dx2.data)


```

```
tensor(-2.)
tensor(2.)
```

```python
import numpy as np
import torch

x1 = torch.tensor(1.0,requires_grad = True) # x needs to be differentiated
x2 = torch.tensor(2.0, requires_grad = True)

y1 = x1*x2
y2 = x1+x2


# Allow to take the derivative of multiple independent variables at the same time
(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1,inputs = [x1,x2],retain_graph = True)
print(dy1_dx1,dy1_dx2)

# If there are multiple dependent variables, it is equivalent to summing the gradient results of multiple dependent variables
(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])
print(dy12_dx1,dy12_dx2)


```

```
tensor(2.) tensor(1.)
tensor(3.) tensor(2.)
```

```python

```

### Three, use automatic differentiation and optimizer to find the minimum

```python
import numpy as np
import torch

# f(x) = a*x**2 + b*x + the minimum value of c

x = torch.tensor(0.0,requires_grad = True) # x needs to be differentiated
a = torch.tensor(1.0)
b = torch.tensor(-2.0)
c = torch.tensor(1.0)

optimizer = torch.optim.SGD(params=[x],lr = 0.01)


def f(x):
    result = a*torch.pow(x,2) + b*x + c
    return(result)

for i in range(500):
    optimizer.zero_grad()
    y = f(x)
    y.backward()
    optimizer.step()
   
    
print("y=",f(x).data,";","x=",x.data)

```

```
y = tensor(0.); x = tensor(1.0000)
```

```python

```

**If this book is helpful to you and want to encourage the author, remember to add a star‚≠êÔ∏è to this project and share it with your friends üòä!**

If you need to further communicate with the author on the understanding of the content of this book, please leave a message under the public account "Algorithm Food House". The author has limited time and energy and will respond as appropriate.

You can also reply to keywords in the background of the official account: **Add group**, join the reader exchange group and discuss with you.

![ÁÆóÊ≥ïÁæéÈ£üÂ±ãlogo.png](./data/ÁÆóÊ≥ïÁæéÈ£üÂ±ã‰∫åÁª¥Á†Å.jpg)
